{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9cb723-9975-4919-bf3b-701807aae55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths set. Ready to load CSVs.\n",
      "\n",
      "=== Loading LONG CSV ===\n",
      "Loaded LONG: 2,308 rows\n",
      "LONG raw shapes: (2308, 160, 18) (2308, 17) (2308,)\n",
      "\n",
      "=== Loading SHORT CSV ===\n",
      "Loaded SHORT: 2,317 rows\n",
      "SHORT raw shapes: (2317, 160, 18) (2317, 17) (2317,)\n",
      "\n",
      "LONG split -> train: 1384, valid: 462, test: 462\n",
      "SHORT split -> train: 1390, valid: 464, test: 463\n",
      "\n",
      "Unified scaler fitted on LONG_train ∪ SHORT_train.\n",
      "Series mean shape: (18,) std shape: (18,)\n",
      "Snapshot mean shape: (17,) std shape: (17,)\n",
      "Saved LONG datasets.\n",
      "Saved SHORT datasets.\n",
      "Saved unified scaler to scaler_unified.json\n",
      "\n",
      "=== Sanity check on prepared NPZ files ===\n",
      "long_train.npz: X_series=(1384, 160, 18), X_snapshot=(1384, 17), y=(1384,), y_mean=1.8388, y_std=134.9398\n",
      "long_valid.npz: X_series=(462, 160, 18), X_snapshot=(462, 17), y=(462,), y_mean=10.3098, y_std=146.8674\n",
      "long_test.npz: X_series=(462, 160, 18), X_snapshot=(462, 17), y=(462,), y_mean=9.3164, y_std=175.2634\n",
      "short_train.npz: X_series=(1390, 160, 18), X_snapshot=(1390, 17), y=(1390,), y_mean=0.4201, y_std=101.3202\n",
      "short_valid.npz: X_series=(464, 160, 18), X_snapshot=(464, 17), y=(464,), y_mean=-1.0627, y_std=96.6907\n",
      "short_test.npz: X_series=(463, 160, 18), X_snapshot=(463, 17), y=(463,), y_mean=8.3856, y_std=134.1407\n",
      "\n",
      "Done. Datasets are ready with a **single unified scaler**. You can now train the unified LONG+SHORT classifier notebook on these NPZ files.\n"
     ]
    }
   ],
   "source": [
    "# --- data_prep.py ---\n",
    "# Prepare LONG and SHORT trade datasets for deep learning.\n",
    "#\n",
    "# Design:\n",
    "# - Outer split is **time-based**:\n",
    "#     * sort by timestamp\n",
    "#     * earliest ~80% = dev (train+valid), latest ~20% = test\n",
    "# - Inner split within dev is **random**:\n",
    "#     * train ≈ 60% of all data\n",
    "#     * valid ≈ 20% of all data\n",
    "#     * test ≈ 20% of all data (strictly later in time)\n",
    "#\n",
    "# - LONG and SHORT are split separately, but:\n",
    "#     * a **single unified scaler** is fitted on (LONG_train ∪ SHORT_train)\n",
    "#     * the same scaler is applied to all splits of both directions\n",
    "#\n",
    "# Outputs (per direction):\n",
    "#   prepared/\n",
    "#       long_train.npz / long_valid.npz / long_test.npz\n",
    "#       short_train.npz / short_valid.npz / short_test.npz\n",
    "#\n",
    "# Each NPZ contains:\n",
    "#   X_series:   (N, 160, 18)   # time-series features (standardized)\n",
    "#   X_snapshot: (N, 17)        # snapshot features (standardized)\n",
    "#   y:          (N,)           # net PnL in USD (float32)\n",
    "#   sample_id, timestamp, direction, exit_reason: string arrays\n",
    "#\n",
    "# Also saves:\n",
    "#   prepared/scaler_unified.json\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------- Global constants --------------------\n",
    "\n",
    "SEED = 42  # base seed for reproducibility\n",
    "\n",
    "# Window length (time steps) and feature channels must match the logger in NinjaTrader.\n",
    "T = 160  # time steps\n",
    "F = 18   # time-series feature channels\n",
    "S = 17   # snapshot features\n",
    "\n",
    "# CSV inputs (exported by your NinjaTrader strategy)\n",
    "DATA_DIR = Path(\"./data\")  # change if needed\n",
    "LONG_CSV  = DATA_DIR / \"STDL_LongLog_20_row_sample.csv\"\n",
    "SHORT_CSV = DATA_DIR / \"STDL_ShortLog_20_row_sample.csv\"\n",
    "\n",
    "# Output directory for prepared datasets\n",
    "OUT_DIR = Path(\"./prepared\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Base names for the 18 time-series feature channels (must match CSV header)\n",
    "SERIES_BASES = [\n",
    "    \"series_log_ret_1\",\n",
    "    \"series_tr_range_pct\",\n",
    "    \"series_vol_rel20\",\n",
    "    \"series_atr14_pct\",\n",
    "    \"series_dist_rma1_atr\",\n",
    "    \"series_dist_rma2_atr\",\n",
    "    \"series_rma1_slope_norm\",\n",
    "    \"series_rma2_slope_norm\",\n",
    "    \"series_dist_st_upper_atr\",\n",
    "    \"series_dist_st_lower_atr\",\n",
    "    \"series_st_bandwidth_atr\",\n",
    "    \"series_rsi14_scaled\",\n",
    "    \"series_macd_hist\",\n",
    "    \"series_body_to_range\",\n",
    "    \"series_dist_ema20_atr\",\n",
    "    \"series_dist_ema50_atr\",\n",
    "    \"series_ema20_slope_norm\",\n",
    "    \"series_ema50_slope_norm\",\n",
    "]\n",
    "\n",
    "# Snapshot feature names (order matters; will be standardized per-feature)\n",
    "SNAPSHOT_COLS = [\n",
    "    \"order_offset_ticks\", \"minutes_from_open\",\n",
    "    \"dow_mon\", \"dow_tue\", \"dow_wed\", \"dow_thu\", \"dow_fri\",\n",
    "    \"vol_regime20\", \"ret_since_session_open_pct\",\n",
    "    \"bars_since_last_st_flip\", \"st_flips_today\",\n",
    "    \"htf15_dist_ema50_atr\", \"htf15_dist_ema200_atr\", \"htf15_rsi14_scaled\",\n",
    "    \"htf60_dist_ema50_atr\", \"htf60_dist_ema200_atr\", \"htf60_rsi14_scaled\",\n",
    "]\n",
    "\n",
    "# Meta and label columns (kept for reference; labels used for y)\n",
    "META_COLS = [\n",
    "    \"sample_id\", \"timestamp\", \"instrument\", \"session_date\", \"direction\"\n",
    "]\n",
    "LABEL_COLS = [\n",
    "    \"filled\", \"exit_reason\", \"entry_price\", \"exit_price\", \"quantity\",\n",
    "    \"pnl_ticks_gross\", \"pnl_usd_gross\", \"pnl_ticks_net\", \"pnl_usd_net\"\n",
    "]\n",
    "\n",
    "print(\"Paths set. Ready to load CSVs.\")\n",
    "\n",
    "\n",
    "# -------------------- Helper functions --------------------\n",
    "\n",
    "def build_series_columns():\n",
    "    \"\"\"\n",
    "    Return ordered list of all series columns as they appear in CSV:\n",
    "       feature1_1..feature1_160, feature2_1..feature2_160, ..., feature18_1..feature18_160\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    for base in SERIES_BASES:\n",
    "        cols.extend([f\"{base}_{i}\" for i in range(1, T + 1)])\n",
    "    return cols\n",
    "\n",
    "\n",
    "def load_direction_csv(csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single-direction CSV (already filtered to LONG or SHORT by NinjaTrader).\n",
    "    - Parse timestamps\n",
    "    - Enforce presence of required columns\n",
    "    - Sort by time ascending (for time-based splitting)\n",
    "    \"\"\"\n",
    "    parse_dates = [\"timestamp\", \"session_date\"]\n",
    "    df = pd.read_csv(csv_path, low_memory=False, parse_dates=parse_dates)\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    missing = []\n",
    "    for c in META_COLS + SNAPSHOT_COLS + LABEL_COLS:\n",
    "        if c not in df.columns:\n",
    "            missing.append(c)\n",
    "\n",
    "    # Check all time-series columns\n",
    "    for c in build_series_columns():\n",
    "        if c not in df.columns:\n",
    "            missing.append(c)\n",
    "\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"Missing columns in {csv_path}: \"\n",
    "            f\"{missing[:10]}{'...' if len(missing) > 10 else ''}\"\n",
    "        )\n",
    "\n",
    "    # Sort by time (chronological ascending)\n",
    "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def dataframe_to_arrays(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Extract tensors (X_series, X_snapshot), labels y, and metadata arrays.\n",
    "       X_series   shape: (N, T, F)\n",
    "       X_snapshot shape: (N, S)\n",
    "       y          shape: (N,) - net PnL in USD (float)\n",
    "    \"\"\"\n",
    "    # Build series tensor by stacking per-channel sequences of length T\n",
    "    series_list = []\n",
    "    for base in SERIES_BASES:\n",
    "        cols = [f\"{base}_{i}\" for i in range(1, T + 1)]\n",
    "        chan = df[cols].to_numpy(dtype=np.float64)   # (N, T)\n",
    "        series_list.append(chan[:, :, None])         # (N, T, 1)\n",
    "    X_series = np.concatenate(series_list, axis=2)   # (N, T, F)\n",
    "\n",
    "    # Snapshot features\n",
    "    X_snapshot = df[SNAPSHOT_COLS].to_numpy(dtype=np.float64)  # (N, S)\n",
    "\n",
    "    # Labels: net PnL in USD\n",
    "    y = df[\"pnl_usd_net\"].to_numpy(dtype=np.float64)  # (N,)\n",
    "\n",
    "    # Meta for reference\n",
    "    meta = {\n",
    "        \"sample_id\":  df[\"sample_id\"].astype(str).to_numpy(),\n",
    "        \"timestamp\":  df[\"timestamp\"].astype(str).to_numpy(),\n",
    "        \"direction\":  df.get(\"direction\", pd.Series([\"NA\"] * len(df))).astype(str).to_numpy(),\n",
    "        \"exit_reason\": df[\"exit_reason\"].astype(str).to_numpy(),\n",
    "    }\n",
    "    return X_series, X_snapshot, y, meta\n",
    "\n",
    "\n",
    "def time_then_random_split_indices(\n",
    "    N: int,\n",
    "    train_ratio_total: float = 0.6,\n",
    "    valid_ratio_total: float = 0.2,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Time-based outer split + random inner split:\n",
    "\n",
    "    - Data assumed sorted by time ascending: index 0 -> oldest trade, N-1 -> newest trade.\n",
    "    - dev_frac = train_ratio_total + valid_ratio_total (e.g. 0.8)\n",
    "    - dev part = earliest dev_frac of data\n",
    "    - test part = latest (1 - dev_frac) of data\n",
    "    - Inside dev, do a random split to get overall ~train_ratio_total / valid_ratio_total.\n",
    "\n",
    "    Returns:\n",
    "        train_idx, valid_idx, test_idx   (each is a 1D int array of indices)\n",
    "    \"\"\"\n",
    "    dev_frac = train_ratio_total + valid_ratio_total\n",
    "    if dev_frac <= 0.0 or dev_frac >= 1.0:\n",
    "        raise ValueError(\"dev_frac must be between 0 and 1 (exclusive).\")\n",
    "\n",
    "    dev_size = int(round(N * dev_frac))\n",
    "    dev_size = max(min(dev_size, N), 2)  # at least 2 samples in dev\n",
    "\n",
    "    test_idx = np.arange(dev_size, N)  # strictly later in time\n",
    "    dev_idx = np.arange(0, dev_size)   # earlier period\n",
    "\n",
    "    # Random split within dev\n",
    "    rng = np.random.default_rng(seed)\n",
    "    perm = rng.permutation(dev_size)\n",
    "\n",
    "    # Train share within dev\n",
    "    train_frac_dev = train_ratio_total / dev_frac\n",
    "    n_train = int(round(dev_size * train_frac_dev))\n",
    "    n_train = max(min(n_train, dev_size - 1), 1)  # at least 1 train, 1 valid\n",
    "\n",
    "    train_idx = perm[:n_train]\n",
    "    valid_idx = perm[n_train:]\n",
    "\n",
    "    return train_idx, valid_idx, test_idx\n",
    "\n",
    "\n",
    "def fit_scalers(X_series_train: np.ndarray, X_snapshot_train: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute per-channel z-score parameters.\n",
    "    Series: mean/std per channel across (samples × time).\n",
    "    Snapshot: mean/std per feature across samples.\n",
    "    \"\"\"\n",
    "    mu_series = X_series_train.mean(axis=(0, 1))  # (F,)\n",
    "    std_series = X_series_train.std(axis=(0, 1))  # (F,)\n",
    "    std_series = np.where(std_series < 1e-8, 1.0, std_series)\n",
    "\n",
    "    mu_snap = X_snapshot_train.mean(axis=0)       # (S,)\n",
    "    std_snap = X_snapshot_train.std(axis=0)       # (S,)\n",
    "    std_snap = np.where(std_snap < 1e-8, 1.0, std_snap)\n",
    "\n",
    "    return mu_series, std_series, mu_snap, std_snap\n",
    "\n",
    "\n",
    "def apply_scalers(\n",
    "    X_series: np.ndarray,\n",
    "    X_snapshot: np.ndarray,\n",
    "    mu_series,\n",
    "    std_series,\n",
    "    mu_snap,\n",
    "    std_snap,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply z-score scaling with broadcasting.\n",
    "    \"\"\"\n",
    "    Xs = (X_series - mu_series.reshape(1, 1, -1)) / std_series.reshape(1, 1, -1)\n",
    "    Xp = (X_snapshot - mu_snap.reshape(1, -1)) / std_snap.reshape(1, -1)\n",
    "    return Xs.astype(np.float32), Xp.astype(np.float32)\n",
    "\n",
    "\n",
    "def save_npz(path: Path, X_series, X_snapshot, y, meta: dict):\n",
    "    \"\"\"\n",
    "    Save arrays to compressed NPZ. Meta fields are saved as arrays of strings.\n",
    "    \"\"\"\n",
    "    np.savez_compressed(\n",
    "        path,\n",
    "        X_series=X_series,\n",
    "        X_snapshot=X_snapshot,\n",
    "        y=y.astype(np.float32),\n",
    "        sample_id=meta[\"sample_id\"],\n",
    "        timestamp=meta[\"timestamp\"],\n",
    "        direction=meta[\"direction\"],\n",
    "        exit_reason=meta[\"exit_reason\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def save_scaler_json(path: Path, mu_series, std_series, mu_snap, std_snap):\n",
    "    \"\"\"\n",
    "    Save scaler params so you can reproduce the exact standardization at inference.\n",
    "    \"\"\"\n",
    "    obj = {\n",
    "        \"T\": T,\n",
    "        \"F\": F,\n",
    "        \"S\": S,\n",
    "        \"series_bases\": SERIES_BASES,\n",
    "        \"snapshot_cols\": SNAPSHOT_COLS,\n",
    "        \"mu_series\": mu_series.tolist(),\n",
    "        \"std_series\": std_series.tolist(),\n",
    "        \"mu_snapshot\": mu_snap.tolist(),\n",
    "        \"std_snapshot\": std_snap.tolist(),\n",
    "    }\n",
    "    path.write_text(json.dumps(obj, indent=2))\n",
    "\n",
    "\n",
    "# -------------------- Load LONG and SHORT raw CSVs --------------------\n",
    "\n",
    "print(\"\\n=== Loading LONG CSV ===\")\n",
    "df_long = load_direction_csv(LONG_CSV)\n",
    "print(f\"Loaded LONG: {len(df_long):,} rows\")\n",
    "X_series_L, X_snap_L, y_L, meta_L = dataframe_to_arrays(df_long)\n",
    "print(\"LONG raw shapes:\", X_series_L.shape, X_snap_L.shape, y_L.shape)\n",
    "\n",
    "print(\"\\n=== Loading SHORT CSV ===\")\n",
    "df_short = load_direction_csv(SHORT_CSV)\n",
    "print(f\"Loaded SHORT: {len(df_short):,} rows\")\n",
    "X_series_S, X_snap_S, y_S, meta_S = dataframe_to_arrays(df_short)\n",
    "print(\"SHORT raw shapes:\", X_series_S.shape, X_snap_S.shape, y_S.shape)\n",
    "\n",
    "\n",
    "# -------------------- Time-based split for each direction --------------------\n",
    "\n",
    "# LONG\n",
    "idx_tr_L, idx_va_L, idx_te_L = time_then_random_split_indices(\n",
    "    len(df_long),\n",
    "    train_ratio_total=0.6,\n",
    "    valid_ratio_total=0.2,\n",
    "    seed=SEED + 1,\n",
    ")\n",
    "print(\n",
    "    f\"\\nLONG split -> \"\n",
    "    f\"train: {len(idx_tr_L)}, valid: {len(idx_va_L)}, test: {len(idx_te_L)}\"\n",
    ")\n",
    "\n",
    "# SHORT\n",
    "idx_tr_S, idx_va_S, idx_te_S = time_then_random_split_indices(\n",
    "    len(df_short),\n",
    "    train_ratio_total=0.6,\n",
    "    valid_ratio_total=0.2,\n",
    "    seed=SEED + 2,\n",
    ")\n",
    "print(\n",
    "    f\"SHORT split -> \"\n",
    "    f\"train: {len(idx_tr_S)}, valid: {len(idx_va_S)}, test: {len(idx_te_S)}\"\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------- Fit a unified scaler on LONG_train ∪ SHORT_train --------------------\n",
    "\n",
    "X_series_train_all = np.concatenate(\n",
    "    [X_series_L[idx_tr_L], X_series_S[idx_tr_S]], axis=0\n",
    ")\n",
    "X_snap_train_all = np.concatenate(\n",
    "    [X_snap_L[idx_tr_L], X_snap_S[idx_tr_S]], axis=0\n",
    ")\n",
    "\n",
    "mu_series, sd_series, mu_snap, sd_snap = fit_scalers(\n",
    "    X_series_train_all,\n",
    "    X_snap_train_all,\n",
    ")\n",
    "\n",
    "print(\"\\nUnified scaler fitted on LONG_train ∪ SHORT_train.\")\n",
    "print(\"Series mean shape:\", mu_series.shape, \"std shape:\", sd_series.shape)\n",
    "print(\"Snapshot mean shape:\", mu_snap.shape, \"std shape:\", sd_snap.shape)\n",
    "\n",
    "# -------------------- Apply unified scaler and save NPZs --------------------\n",
    "\n",
    "# LONG\n",
    "Xs_tr_L, Xp_tr_L = apply_scalers(\n",
    "    X_series_L[idx_tr_L], X_snap_L[idx_tr_L],\n",
    "    mu_series, sd_series, mu_snap, sd_snap\n",
    ")\n",
    "Xs_va_L, Xp_va_L = apply_scalers(\n",
    "    X_series_L[idx_va_L], X_snap_L[idx_va_L],\n",
    "    mu_series, sd_series, mu_snap, sd_snap\n",
    ")\n",
    "Xs_te_L, Xp_te_L = apply_scalers(\n",
    "    X_series_L[idx_te_L], X_snap_L[idx_te_L],\n",
    "    mu_series, sd_series, mu_snap, sd_snap\n",
    ")\n",
    "\n",
    "save_npz(\n",
    "    OUT_DIR / \"long_train.npz\",\n",
    "    Xs_tr_L,\n",
    "    Xp_tr_L,\n",
    "    y_L[idx_tr_L],\n",
    "    {k: v[idx_tr_L] for k, v in meta_L.items()},\n",
    ")\n",
    "save_npz(\n",
    "    OUT_DIR / \"long_valid.npz\",\n",
    "    Xs_va_L,\n",
    "    Xp_va_L,\n",
    "    y_L[idx_va_L],\n",
    "    {k: v[idx_va_L] for k, v in meta_L.items()},\n",
    ")\n",
    "save_npz(\n",
    "    OUT_DIR / \"long_test.npz\",\n",
    "    Xs_te_L,\n",
    "    Xp_te_L,\n",
    "    y_L[idx_te_L],\n",
    "    {k: v[idx_te_L] for k, v in meta_L.items()},\n",
    ")\n",
    "\n",
    "print(\"Saved LONG datasets.\")\n",
    "\n",
    "# SHORT\n",
    "Xs_tr_S, Xp_tr_S = apply_scalers(\n",
    "    X_series_S[idx_tr_S], X_snap_S[idx_tr_S],\n",
    "    mu_series, sd_series, mu_snap, sd_snap\n",
    ")\n",
    "Xs_va_S, Xp_va_S = apply_scalers(\n",
    "    X_series_S[idx_va_S], X_snap_S[idx_va_S],\n",
    "    mu_series, sd_series, mu_snap, sd_snap\n",
    ")\n",
    "Xs_te_S, Xp_te_S = apply_scalers(\n",
    "    X_series_S[idx_te_S], X_snap_S[idx_te_S],\n",
    "    mu_series, sd_series, mu_snap, sd_snap\n",
    ")\n",
    "\n",
    "save_npz(\n",
    "    OUT_DIR / \"short_train.npz\",\n",
    "    Xs_tr_S,\n",
    "    Xp_tr_S,\n",
    "    y_S[idx_tr_S],\n",
    "    {k: v[idx_tr_S] for k, v in meta_S.items()},\n",
    ")\n",
    "save_npz(\n",
    "    OUT_DIR / \"short_valid.npz\",\n",
    "    Xs_va_S,\n",
    "    Xp_va_S,\n",
    "    y_S[idx_va_S],\n",
    "    {k: v[idx_va_S] for k, v in meta_S.items()},\n",
    ")\n",
    "save_npz(\n",
    "    OUT_DIR / \"short_test.npz\",\n",
    "    Xs_te_S,\n",
    "    Xp_te_S,\n",
    "    y_S[idx_te_S],\n",
    "    {k: v[idx_te_S] for k, v in meta_S.items()},\n",
    ")\n",
    "\n",
    "print(\"Saved SHORT datasets.\")\n",
    "\n",
    "# Save unified scaler\n",
    "save_scaler_json(\n",
    "    OUT_DIR / \"scaler_unified.json\",\n",
    "    mu_series,\n",
    "    sd_series,\n",
    "    mu_snap,\n",
    "    sd_snap,\n",
    ")\n",
    "print(\"Saved unified scaler to scaler_unified.json\")\n",
    "\n",
    "\n",
    "# -------------------- Quick sanity checks --------------------\n",
    "\n",
    "def npz_info(path: Path):\n",
    "    with np.load(path) as z:\n",
    "        Xs = z[\"X_series\"]\n",
    "        Xp = z[\"X_snapshot\"]\n",
    "        y  = z[\"y\"]\n",
    "        print(\n",
    "            f\"{path.name}: \"\n",
    "            f\"X_series={Xs.shape}, X_snapshot={Xp.shape}, y={y.shape}, \"\n",
    "            f\"y_mean={y.mean():.4f}, y_std={y.std():.4f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"\\n=== Sanity check on prepared NPZ files ===\")\n",
    "for p in [\n",
    "    OUT_DIR / \"long_train.npz\",\n",
    "    OUT_DIR / \"long_valid.npz\",\n",
    "    OUT_DIR / \"long_test.npz\",\n",
    "    OUT_DIR / \"short_train.npz\",\n",
    "    OUT_DIR / \"short_valid.npz\",\n",
    "    OUT_DIR / \"short_test.npz\",\n",
    "]:\n",
    "    npz_info(p)\n",
    "\n",
    "print(\"\\nDone. Datasets are ready with a **single unified scaler**. \"\n",
    "      \"You can now train the unified LONG+SHORT classifier notebook on these NPZ files.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
